{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "epsilon = 1.0 \n",
    "max_epsilon = 1.0 \n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.002\n",
    "num_episodes = 40000\n",
    "max_steps=100\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(454, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "# This cell is used to get to know what values does env.step(action) returns\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "print(env.step(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [  5.58336394   6.38381137   4.93807102   5.1701929    9.6220697\n",
      "   -4.08646785]\n",
      " [  6.82631508  10.93074358   7.42314976   9.66857054  14.11880599\n",
      "    1.42431572]\n",
      " ...\n",
      " [ -1.81753791  -1.79534923  -1.89318544  10.13666893  -9.67404462\n",
      "  -10.61075952]\n",
      " [ -3.57892254   9.27198853  -3.57156717  -3.70595747 -10.98570409\n",
      "  -12.2796812 ]\n",
      " [ -0.05760128   2.21942109   0.12671872  18.78280857  -1.80865134\n",
      "   -3.2204736 ]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    epi_reward=0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state,:])\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
    "\n",
    "        state = new_state\n",
    "        epi_reward += reward\n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(epi_reward)\n",
    "\n",
    "print(Q)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Here we can see that the net rewards remain about the same after about 5 to 6 thousand episodes so we can also stop at that but I've done more episodes just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 : -15.283599999999963\n",
      "2000 : 0.34590000000000026\n",
      "3000 : 0.6829000000000011\n",
      "4000 : 0.7258000000000014\n",
      "5000 : 0.7447000000000022\n",
      "6000 : 0.7693000000000004\n",
      "7000 : 0.7425000000000009\n",
      "8000 : 0.7566000000000006\n",
      "9000 : 0.7371000000000011\n",
      "10000 : 0.7503000000000002\n",
      "11000 : 0.7534000000000006\n",
      "12000 : 0.7384000000000011\n",
      "13000 : 0.7509000000000011\n",
      "14000 : 0.7449000000000013\n",
      "15000 : 0.7441000000000015\n",
      "16000 : 0.7365000000000013\n",
      "17000 : 0.7618000000000004\n",
      "18000 : 0.7509000000000016\n",
      "19000 : 0.7454000000000008\n",
      "20000 : 0.7364000000000012\n",
      "21000 : 0.7341999999999995\n",
      "22000 : 0.7378000000000021\n",
      "23000 : 0.7423000000000008\n",
      "24000 : 0.746000000000001\n",
      "25000 : 0.7428000000000016\n",
      "26000 : 0.7676000000000017\n",
      "27000 : 0.7466999999999997\n",
      "28000 : 0.758000000000001\n",
      "29000 : 0.7279000000000004\n",
      "30000 : 0.7509000000000005\n",
      "31000 : 0.7460000000000021\n",
      "32000 : 0.7452000000000012\n",
      "33000 : 0.7399000000000011\n",
      "34000 : 0.752000000000001\n",
      "35000 : 0.745100000000001\n",
      "36000 : 0.743200000000002\n",
      "37000 : 0.7251000000000029\n",
      "38000 : 0.7297000000000018\n",
      "39000 : 0.7453000000000003\n",
      "40000 : 0.7238000000000011\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand= np.split(np.array(rewards), num_episodes/1000)\n",
    "count = 1000\n",
    "for r in rewards_per_thousand:\n",
    "    print( count, \":\", str(sum(r/10000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(3):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        action = np.argmax(Q[state,:])\n",
    "        \n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(env.render())\n",
    "            # we can see if the last action displayed in the cell output is dropoff then we have succsessfully completed the task\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
