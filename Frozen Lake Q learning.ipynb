{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Here I've used version 1 of frozen lake as v0 was out of date. Most things remain the same except some like in this env.reset also gives additional values for debugging purposes. I have decided to keep the slippery mode ON for this excersise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='ansi', desc=None, map_name=\"4x4\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Alpha is the learning rate which is used to take the weighted average of the known Q-value and the new learned value. I have decided to keep is a little on the lower side. Gamma is the duscount rate which decides how far into the future do we consider rewards for maximizing the expected return. epsilon is used for exploitaion vs exploration tradeoff. decay rate is less as for the first few episodes I want to explore more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2  \n",
    "gamma = 0.99  \n",
    "epsilon = 1.0 \n",
    "max_epsilon = 1.0 \n",
    "min_epsilon = 0.01 \n",
    "decay_rate = 0.005\n",
    "num_episodes = 20000\n",
    "max_steps=100\n",
    "rewards = []\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Here I've found out another issue with the new version. env.step(action) returns one extra bool value than expected. After some research I found out that it was something related to the possiblity that the episode was truncated due to some condition. Due to lack of complete knowledge about this I've not used truncated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step=0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_rewards =0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "            \n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
    "\n",
    "        total_rewards += reward\n",
    "\n",
    "        state = new_state\n",
    "        if( done == True):\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_thousand= np.split(np.array(rewards), num_episodes/1000)\n",
    "count = 1000\n",
    "for r in rewards_per_thousand:\n",
    "    print( count, \":\", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "In running the final test I've only printed the final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(3):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        action = np.argmax(Q[state,:])\n",
    "        \n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(env.render())\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
